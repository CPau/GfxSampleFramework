/*	http://www.reedbeta.com/blog/understanding-bcn-texture-compression-formats/
	http://sjbrown.co.uk/2006/01/19/dxt-compression-techniques/
	http://fileadmin.cs.lth.se/cs/education/edan35/lectures/l8-texcomp.pdf
	http://developer.download.nvidia.com/compute/cuda/1.1-Beta/x86_website/projects/dxtc/doc/cuda_dxtc.pdf
	https://pdfs.semanticscholar.org/presentation/9410/6e86ee70426b81b7f64d392a068c5ebda06a.pdf
	http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.215.7942&rep=rep1&type=pdf
	http://fileadmin.cs.lth.se/graphics/research/papers/gputc2006/thesis.pdf

	\todo
	- Optimize HQ shader.
		- Parallel computation of the covariance matrix: 
			Each of the 6 terms requires looping over the src block and computing the sum of (_src[i][_x] - _avg[_x]) * (_src[i][_y] - _avg[_y[),
			So, make 3 arrays of 16 float for _src[i][x] - _avg[x] (with x = 0,1,2)
			Then either:
				- 6 parallel threads to compute the multiplication/sum part for each of the matrix values. No extra LDS required but threads underutilized).
				- 6 steps: compute the mul part to an LDS buffer of 16 floats, then sum via parallel reduction. Requires more LDS but better utilizes threads.
				  Could potentially 'interleave' the PR part? More complicated.
			Subsequent steps can't be parallelized! Or would require atomics, so probably not worth it.
	- Systematic error analysis: http://homepages.inf.ed.ac.uk/rbf/CVonline/LOCAL_COPIES/VELDHUIZEN/node18.html
		- Compare vs original, also measure error in an image generated by an offline compression tool.
	- Alpha support + fast path for solid color blocks (see here https://pdfs.semanticscholar.org/presentation/9410/6e86ee70426b81b7f64d392a068c5ebda06a.pdf)
	- 4 blocks at once for better thread utilization (8x8 vs 4x4)?

	notes:
	- The HQ implementation is basically a copy of stb_dxt but without the refinement step
*/
#include "shaders/def.glsl"

#define ENDPOINT_HQ 1  // use PCA to find endpoints
#define INDEX_HQ    0  // use Euclidean distance to find indices

#define COV_OPTIM 1

uniform sampler2D txSrc;

layout(std430) restrict writeonly buffer _bfDst
{
	uvec2 bfDst[];
};

uint Pack_RGB565(in vec3 _rgb)
{
	uint ret = 0;
	ret = bitfieldInsert(ret, uint(_rgb.r * 31.0), 11, 5);
	ret = bitfieldInsert(ret, uint(_rgb.g * 63.0), 5,  6);
	ret = bitfieldInsert(ret, uint(_rgb.b * 31.0), 0,  5);
	return ret;
}
vec3 Unpack_RGB565(in uint _565)
{
	vec3 ret;
	ret.r = float(bitfieldExtract(_565, 11, 5)) / 31.0;
	ret.g = float(bitfieldExtract(_565, 5,  6)) / 63.0;
	ret.b = float(bitfieldExtract(_565, 0,  5)) / 31.0;
	return ret;
}

shared vec3 s_srcBlock[16];   // raw block texels
shared uint s_dstIndices[16]; // per-texel palette indices

#if COV_OPTIM
	shared vec3  s_covFactors[16];
	shared float s_covMatrix[6];
#endif

float cov(in int _x, in int _y, in vec3 _avg)
{
	float ret = 0.0;
	for (int i = 0; i < 16; ++i) {
		ret += (s_srcBlock[i][_x] - _avg[_x]) * (s_srcBlock[i][_y] - _avg[_y]);
	}
	ret *= 1.0/16.0;
	return ret;
} 

void main()
{
 // gather block texels (each thread reads 1 texel)
	ivec2 iuv = ivec2(gl_WorkGroupID.xy * 4 + gl_LocalInvocationID.xy);
	s_srcBlock[gl_LocalInvocationIndex] = texelFetch(txSrc, iuv, 0).rgb;
	groupMemoryBarrier();

 // find endpoints
	vec3 ep0 = vec3(1.0);
	vec3 ep1 = vec3(0.0);
	#if ENDPOINT_HQ
	 // slow, high-quality: use principal component analysis
	 // \todo build the covariance matrix via parallel reduction
	 // http://www.visiondummy.com/2014/04/geometric-interpretation-covariance-matrix/
		vec3 avg = s_srcBlock[0];
	 	ep0 = ep1 = s_srcBlock[0];
		for (int i = 1; i < 16; ++i) {
			ep0  = min(ep0, s_srcBlock[i]);
			ep1  = max(ep1, s_srcBlock[i]);
			avg += s_srcBlock[i];
		}
		avg *= 1.0/16.0;

	 // generate covariance matrix
		#if COV_OPTIM
			s_covFactors[gl_LocalInvocationIndex] = s_srcBlock[gl_LocalInvocationIndex] - avg;

			#if 1
		 	 // use 6 threads to generate the matrix elements
			 // \todo this is slower than the unoptimized version, maybe due to bank conflicts?
			 // you're also not removing the loop, only some of the computation inside it, if you accept to do the subtraction in the loop you could avoid a memory barrier.
				const uvec2 covIndices[6] = { 
					uvec2(0, 0),
					uvec2(0, 1),
					uvec2(0, 2),
					uvec2(1, 1),
					uvec2(1, 2),
					uvec2(2, 2)
					}; 
				if (gl_LocalInvocationIndex < 6) {
					groupMemoryBarrier(); // only these threads need to wait for s_coFactors
					float ret = 0.0;
					uvec2 ci = covIndices[gl_LocalInvocationIndex];
					for (int i = 0; i < 16; ++i) {
						ret += s_covFactors[i][ci.x] * s_covFactors[i][ci.y];
					}
					s_covMatrix[gl_LocalInvocationIndex] = ret * 1.0/16.0;
				}
				groupMemoryBarrier();
				
				mat3  C = mat3(
					s_covMatrix[0], s_covMatrix[1], s_covMatrix[2],
					s_covMatrix[1], s_covMatrix[3], s_covMatrix[4],
					s_covMatrix[2], s_covMatrix[4], s_covMatrix[5]
					);
			#else
			#endif
		#else
			float cRR = cov(0, 0, avg);
			float cRG = cov(0, 1, avg);
			float cRB = cov(0, 2, avg);
			float cGG = cov(1, 1, avg);
			float cGB = cov(1, 2, avg);
			float cBB = cov(2, 2, avg);
			mat3  C = mat3(
				cRR, cRG, cRB,
				cRG, cGG, cGB,
				cRB, cGB, cBB
				);
		#endif

	 // find endpoints
		vec3 vf = abs(ep1 - ep0);
		for (int i = 0; i < 16; ++i) {
			float x = dot(vf, C[0]);
			float y = dot(vf, C[1]);
			float z = dot(vf, C[2]);
			vf = vec3(x, y, z);
		}
		float vflen = length2(vf);
		if (vflen > 1e-4) {
			vf /= sqrt(vflen);
		}

		float mind, maxd;
		mind = maxd = dot(vf, s_srcBlock[0]);
		for (int i = 1; i < 16; ++i) {
			float d = dot(vf, s_srcBlock[i]);
			if (d < mind) {
				ep0 = s_srcBlock[i];
				mind = d;
			}
			if (d > maxd) {
				ep1 = s_srcBlock[i];
				maxd = d;
			}
		}
	#else
	 // fast, low-quality: find the color space bounding box, endpoints are min/max
	 // \todo atomicMin/atomicMax?
	 	ep0 = ep1 = s_srcBlock[0];
		for (int i = 1; i < 16; ++i) {
			ep0 = min(ep0, s_srcBlock[i]);
			ep1 = max(ep1, s_srcBlock[i]);
		}
	#endif
	
 // export endpoints (type 1)
	uvec2 dst;
	uint ep0i = Pack_RGB565(ep0);	
	uint ep1i = Pack_RGB565(ep1);
	dst[0] = 0;
	if (ENDPOINT_HQ == 1 && ep0i > ep1i) {
		dst[0] = bitfieldInsert(dst[0], ep0i, 0,  16);
		dst[0] = bitfieldInsert(dst[0], ep1i, 16, 16);
		vec3 tmp = ep1;
		ep1 = ep0;
		ep0 = tmp;
	} else {
		dst[0] = bitfieldInsert(dst[0], ep1i, 0,  16);
		dst[0] = bitfieldInsert(dst[0], ep0i, 16, 16);
	}

 // find palette indices per texel
	#if INDEX_HQ
	{
	 // square euclidean distance
		vec3 palette[4];
		palette[0] = ep1;
		palette[1] = ep0;
		palette[2] = 2.0/3.0 * palette[0] + 1.0/3.0 * palette[1];
		palette[3] = 1.0/3.0 * palette[0] + 2.0/3.0 * palette[1];
		#if 1
		 // pack/unpack the palette values = quantize palette so that texel indices are generated from the final result
		 // \todo it's not clear that this significantly improves the quality
			for (int i = 0; i < 4; ++i) {
				palette[i] = Unpack_RGB565(Pack_RGB565(palette[i]));
			}
		#endif

		int idx = 0;
		float minErr = 999.0;
		for (int i = 0; i < 4; ++i) {
			float err = length2(s_srcBlock[gl_LocalInvocationIndex] - palette[i]);
			if (err < minErr) {
				minErr = err;
				idx = i;
			}
			s_dstIndices[gl_LocalInvocationIndex] = idx;
		}
	}
	#else
	{
	 // project onto (ep1 - ep0)
		vec3 d = ep1 - ep0;
		float dlen = length(d);
		d /= dlen;
	 	vec3 src = s_srcBlock[gl_LocalInvocationIndex];
		float idx = dot(src - ep0, d) / dlen;

	 // round to nearest palette index
		idx = round(saturate(idx) * 3.0);
		const uvec4 idxMap = uvec4(1, 3, 2, 0);
		s_dstIndices[gl_LocalInvocationIndex] = idxMap[uint(idx)];
	}
	#endif
	groupMemoryBarrier();

 // final block export
	if (gl_LocalInvocationIndex == 0) {
	 // pack palette indices
		dst[1] = 0;
		for (int i = 0; i < 16; ++i) {
			dst[1] = bitfieldInsert(dst[1], s_dstIndices[i], (i * 2), 2);
		}
	
	 // write block data
		bfDst[gl_WorkGroupID.y * gl_NumWorkGroups.x + gl_WorkGroupID.x] = dst;
	}
}
